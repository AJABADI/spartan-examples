== LAMMPS ==

LAMMPS/20170811-intel-2017.u2-GCC-6.2.0-CUDA9.1 is the preferred version for GPGPU operations. There are two binaries in this package: lmp_omp and lmp_mpi. It is almost always worth using lmp_mpi... it offers comparable performance on a single node to the lmp_omp binary, but is also capable of multi node operations.

Because of a compatability issue between intel compilers and MPI/pmi, you will need to add the following command to your slurm scripts before execution when using lmp_mpi:

unset I_MPI_PMI_LIBRARY

In order to use LAMMPS with the GPU module enabled you need to use the -sf and -pk flags, as per the following command: 

mpiexec -np 2 lmp_mpi -sf gpu -pk gpu 1 -in <in.input>

the number after the -pk flag indicates the number of gpu instances you are requesting, and it should line up with the number requested in your gres gpu request. For example, a slurm script with the following line: 

#SBATCH --gres=gpu:2 

is requesting two gpus be allocated to the job. As such, you should be able to scale up to 4 cards per node. Note that your --ntasks value will correlate with a number of processes... but while one will be assigned per cpu by default, multiple processes can be present onboard a single gpu, and mpi capable applications operatign on a single node can communicate between processes directly on the gpu card in this state. You may see an extra efficiency boost in this scenario.
